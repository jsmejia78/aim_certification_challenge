{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prototyping Notebook\n",
        "\n",
        "Prototyping for data loading, retrievals and evals\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY\"\n",
        "assert os.getenv(\"TAVILY_API_KEY\"), \"Missing TAVILY_API_KEY\"\n",
        "assert os.getenv(\"LANGCHAIN_API_KEY\"), \"Missing LANGCHAIN_API_KEY\"\n",
        "assert os.getenv(\"COHERE_API_KEY\"), \"Missing COHERE_API_KEY\"\n",
        "\n",
        "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "langsmith_project_name = \"AIM-CERT-LANGGRAPH-BASELINE\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = langsmith_project_name\n",
        "print(langsmith_project_name)\n",
        "\n",
        "MODE = \"PIPE\"\n",
        "\n",
        "from langgraph_agent import LangGraphAgent, RetrievalEnums\n",
        "\n",
        "Agent = LangGraphAgent(retriever_mode=RetrievalEnums.NAIVE, \n",
        "                       MODE=MODE, \n",
        "                       langchain_project_name= langsmith_project_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Eval Golden Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "with open(f\"eval_golden_dataset.pkl\", \"rb\") as f:\n",
        "    golden_dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import copy\n",
        "import pickle\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate as evaluate_ragas, RunConfig\n",
        "from ragas.metrics import (\n",
        "    LLMContextPrecisionWithReference,\n",
        "    LLMContextRecall,\n",
        "    ContextEntityRecall,\n",
        "    Faithfulness,\n",
        "    ResponseRelevancy\n",
        "    )\n",
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate as evaluate_langsmith\n",
        "from langsmith import Client as ClientLangSmith\n",
        "import numpy as np\n",
        "from retrievers import get_retrieval_chains_and_wrappers_for_evals\n",
        "import pickle\n",
        "import prompts\n",
        "import importlib\n",
        "importlib.reload(prompts)\n",
        "from prompts import get_rag_prompt\n",
        "\n",
        "skip_evaluation = []\n",
        "\n",
        "eval_full_results = {}\n",
        "eval_summary_results = {}\n",
        "eval_langsmith_raw_results = {}\n",
        "eval_langsmith_summary_results = {}\n",
        "\n",
        "\n",
        "# Prepare dataset for this chain\n",
        "golden_dataset_active_copy = copy.deepcopy(golden_dataset)\n",
        "\n",
        "print(f\"data type dataset: {type(golden_dataset_active_copy)}\")\n",
        "\n",
        "# Generate responses for evaluation\n",
        "for test_row in golden_dataset_active_copy:\n",
        "    #response = wrappers[chain_name](test_row.eval_sample.user_input)\n",
        "    response = Agent(test_row.eval_sample.user_input)\n",
        "    Agent.reset_memroy()\n",
        "    test_row.eval_sample.response = response[\"response\"].content\n",
        "\n",
        "    # this needs to change\n",
        "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "\n",
        "evaluation_active_dataset = EvaluationDataset.from_pandas(golden_dataset_active_copy.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "# Run evaluation using only RETRIEVAL metrics\n",
        "print(\"Running RAGAS evaluation...\")\n",
        "eval_result_active = evaluate_ragas(\n",
        "    dataset=evaluation_active_dataset,\n",
        "    metrics=[LLMContextPrecisionWithReference(), LLMContextRecall(), ContextEntityRecall(), Faithfulness(), ResponseRelevancy()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "\n",
        "eval_full_resultsc= eval_result_active\n",
        "\n",
        "# Convert to DataFrames and compute statistics\n",
        "df_eval_result_active = eval_result_active.to_pandas()\n",
        "eval_result_active_means = df_eval_result_active.mean(numeric_only=True)\n",
        "eval_result_active_stds = df_eval_result_active.std(numeric_only=True)\n",
        "\n",
        "eval_summary_results = {\n",
        "    \"means\": eval_result_active_means,\n",
        "    \"stds\": eval_result_active_stds\n",
        "}\n",
        "\n",
        "print(f\"RAGAS Results for pipeline with baseline retrieval:\")\n",
        "print(eval_result_active_means)\n",
        "print(eval_result_active_stds)\n",
        "\n",
        "# save results to pickle in case of errors or crashes\n",
        "pickle_dict_active_ragas_results = { \n",
        "    \"raw_results\": eval_result_active, \n",
        "    \"summary_results\": eval_summary_results }\n",
        "\n",
        "# Save to file\n",
        "with open(f\"cert_eval_ragas_results_for_baseline_retrieval.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pickle_dict_active_ragas_results, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create heatmap visualization\n",
        "def create_heatmap_visualization(eval_summary_results):\n",
        "    \"\"\"\n",
        "    Create a heatmap showing metric performance across chains\n",
        "    \"\"\"\n",
        "    # Prepare data for heatmap\n",
        "    heatmap_data = []\n",
        "    chains = list(eval_summary_results.keys())\n",
        "    metrics = ['llm_context_precision_with_reference', 'context_recall', 'context_entity_recall', 'faithfulness', 'response_relevancy']\n",
        "    \n",
        "    for chain in chains:\n",
        "        means = eval_summary_results[chain][\"means\"]\n",
        "        row_data = [means[metric] for metric in metrics]\n",
        "        heatmap_data.append(row_data)\n",
        "    \n",
        "    # Create DataFrame for heatmap\n",
        "    df_heatmap = pd.DataFrame(\n",
        "        heatmap_data, \n",
        "        index=[chain.replace('_', ' ').title() for chain in chains],\n",
        "        columns=['Context Precision', 'Context Recall', 'Entity Recall', 'Faithfulness', 'Response Relevancy']\n",
        "    )\n",
        "    \n",
        "    # Create the heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df_heatmap, \n",
        "                annot=True, \n",
        "                cmap='RdYlGn',\n",
        "                fmt='.3f',\n",
        "                linewidths=0.5,\n",
        "                cbar_kws={'label': 'Score'},\n",
        "                square=True)\n",
        "    \n",
        "    plt.title('RAG Evaluation Metrics Heatmap\\n(Darker = Better Performance)', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Metrics', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Retrieval Strategies', fontsize=12, fontweight='bold')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return df_heatmap\n",
        "\n",
        "# Generate alternative visualizations\n",
        "print(\"\\nCreating heatmap visualization...\")\n",
        "heatmap_df = create_heatmap_visualization(eval_summary_results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
